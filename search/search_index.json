{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"WPextract - WordPress Site Extractor","text":"<p>WPextract is a tool to create datasets from WordPress sites.</p> <ul> <li>Archives posts, pages, tags, categories, media (including files), comments, and users</li> <li>Uses the WordPress API to guarantee 100% accurate and complete content</li> <li>Resolves internal links and media to IDs</li> <li>Automatically parses multilingual sites to create parallel datasets</li> </ul>"},{"location":"#quickstart","title":"Quickstart","text":"<ol> <li>Install with <code>pipx</code> <pre><code>$ pipx install wpextract\n</code></pre></li> <li>Download site data     <pre><code>$ wpextract dl \"https://example.org\" out_dl\n</code></pre></li> <li>Process into a dataset     <pre><code>$ wpextract extract out_dl out_data\n</code></pre></li> </ol>"},{"location":"#about-wpextract","title":"About WPextract","text":"<p>WPextract was built by Freddy Heppell of the GATE Project at the School of Computer Science, University of Sheffield, originally created to scrape mis/disinformation websites for research.</p>"},{"location":"#citing-wpextract","title":"Citing WPextract","text":"<p>We'd love to hear about your use of our tool, you can email us to let us know! Feel free to create issues and/or pull requests for new features or bugs.</p> <p>If you use this tool in published work, please cite our EMNLP paper:</p> <p>Freddy Heppell, Kalina Bontcheva, and Carolina Scarton. 2023. Analysing State-Backed Propaganda Websites: a New Dataset and Linguistic Study. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5729\u20135741, Singapore. Association for Computational Linguistics.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v100","title":"v1.0.0","text":"<ul> <li>TODO</li> </ul>"},{"location":"license/","title":"License and Acknowledgements","text":"<p>Copyright 2022-24 The University of Sheffield</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0.</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>Portions of this code are derived from WPJsonScraper, which is available under the MIT license. For details, see src/extractor/dl/ORIGINAL_LICENSE.</p>"},{"location":"license/#acknowledgements","title":"Acknowledgements","text":"<p>Development of this project was jointly supported by:</p> <ul> <li>InnovateUK grant number 10039039 (approved under the Horizon Europe Programme as VIGILANT, EU grant agreement number 101073921.</li> <li>InnovateUK grant number 10039055 (approved under the Horizon Europe Programme as vera.ai, EU grant agreement number 101070093).</li> <li>Transnational Access funds from \"SoBigData++: European Integrated Infrastructure for Social Mining and BigData Analytics\" (Horizon 2020 grant agreement number 871042).</li> <li>A University of Sheffield Faculty of Engineering PGR Prize Scholarship</li> </ul> <p>Parts of this program are derived from WPJsonScraper by Mickael Walter.</p>"},{"location":"advanced/library/","title":"Using as a Library","text":"<p>The extractor can also be used as a library instead of on the command line.</p> <p>Typically, you would:</p> <ul> <li>instantiate a <code>WPDownloader</code> instance and call its <code>download</code> method.</li> <li>instantiate a <code>WPExtractor</code> instance and call its <code>extract</code> method. The dataframes can then be accessed as class attributes or exported with the <code>export</code> method.</li> </ul> <p>Examples of usage are available in the CLI scripts in the <code>extractor.cli</code> module.</p>"},{"location":"advanced/library/#downloader","title":"Downloader","text":"<p>Use the <code>extractor.WPDownloader</code> class.</p> <p>Possible customisations include:</p> <ul> <li>Implement highly custom request behaviour by subclassing <code>RequestSession</code> and passing to the <code>session</code> parameter.</li> </ul>"},{"location":"advanced/library/#extractor","title":"Extractor","text":"<p>Use the <code>extractor.WPExtractor</code> class.</p> <p>When using this approach, it's possible to use customised translation pickers by passing subclasses of <code>LanguagePicker</code> to the </p>"},{"location":"advanced/multilingual/","title":"Multilingual Sites","text":"<p>If sites publish in multiple languages and use a plugin to present a list of language versions, wpextract can parse this and add multilingual data in the output dataset.</p>"},{"location":"advanced/multilingual/#extraction-process","title":"Extraction Process","text":"<p>Extracting multilingual data is performed during the extract command. This data isn't available in the WordPress REST API response, so instead must be obtained from scraped HTML.</p> <p>Obtaining the scraped HTML is relatively straightforward, as we already have a list of all posts from the download command.</p> <p>One way this could be scraped is using <code>jq</code> to parse the downloaded posts file and produce a URL list, then <code>wget</code> to download each page:</p> <pre><code>$ cat posts.json | jq -r '.[] | .link' &gt; url_list.txt\n$ touch rejected.log\n$ wget --adjust-extension --input-file=url_list.txt \\\n     --wait 1 --random-wait --force-directories \\\n     --rejected-log=rejected.log\n</code></pre> <p>When running the extract command, pass this directory as the <code>--scrape-root</code> argument. The scrape will be crawled to match URLs to downloaded HTML files following this process.</p>"},{"location":"advanced/multilingual/#supported-plugins","title":"Supported Plugins","text":"<p>wpextract uses an extensible system of parsers to find language picker elements and extract their data.</p> <p>Currently the following plugins are supported:</p>"},{"location":"advanced/multilingual/#polylang","title":"Polylang","text":"<p>Plugin Page \u00b7 Website</p> <p>Supports:</p> <ul> <li> <p>Adding as a widget (e.g. to a sidebar)</p> Example <pre><code>&lt;div id=\"polylang-2\" class=\"widget widget_polylang\"&gt;\n  &lt;ul&gt;\n    &lt;li\n      class=\"lang-item lang-item-18 lang-item-en current-lang lang-item-first\"\n    &gt;\n      &lt;a\n        hreflang=\"en-US\"\n        href=\"https://example.org/current-lang-page/\"\n        lang=\"en-US\"\n      &gt;\n        &lt;img\n          src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAALCAMAAABBPP0LAAAAmVBMVEViZsViZMJiYrf9gnL8eWrlYkjgYkjZYkj8/PujwPybvPz4+PetraBEgfo+fvo3efkydfkqcvj8Y2T8UlL8Q0P8MzP9k4Hz8/Lu7u4DdPj9/VrKysI9fPoDc/EAZ7z7IiLHYkjp6ekCcOTk5OIASbfY/v21takAJrT5Dg6sYkjc3Nn94t2RkYD+y8KeYkjs/v7l5fz0dF22YkjWvcOLAAAAgElEQVR4AR2KNULFQBgGZ5J13KGGKvc/Cw1uPe62eb9+Jr1EUBFHSgxxjP2Eca6AfUSfVlUfBvm1Ui1bqafctqMndNkXpb01h5TLx4b6TIXgwOCHfjv+/Pz+5vPRw7txGWT2h6yO0/GaYltIp5PT1dEpLNPL/SdWjYjAAZtvRPgHJX4Xio+DSrkAAAAASUVORK5CYII=\"\n          alt=\"English\"\n          style=\"width: 16px; height: 11px\"\n          width=\"16\"\n          height=\"11\"\n        /&gt;\n        &lt;span style=\"margin-left: 0.3em\"&gt;English&lt;/span&gt;\n      &lt;/a&gt;\n    &lt;/li&gt;\n    &lt;li class=\"lang-item lang-item-20 lang-item-fr\"&gt;\n      &lt;a\n        hreflang=\"fr-FR\"\n        href=\"https://example.org/fr/translation-page/\"\n        lang=\"fr-FR\"\n      &gt;\n        &lt;img\n          src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAALCAMAAABBPP0LAAAAbFBMVEVzldTg4ODS0tLxDwDtAwDjAADD0uz39/fy8vL3k4nzgna4yOixwuXu7u7s6+zn5+fyd2rvcGPtZljYAABrjNCpvOHrWkxegsqfs93NAADpUUFRd8THAABBa7wnVbERRKa8vLyxsLCoqKigoKClCvcsAAAAXklEQVR4AS3JxUEAQQAEwZo13Mk/R9w5/7UERJCIGIgj5qfRJZEpPyNfCgJTjMR1eRRnJiExFJz5Mf1PokWr/UztIjRGQ3V486u0HO55m634U6dMcf0RNPfkVCTvKjO16xHA8miowAAAAABJRU5ErkJggg==\"\n          alt=\"Fran\u00e7ais\"\n          style=\"width: 16px; height: 11px\"\n          width=\"16\"\n          height=\"11\"\n        /&gt;\n        &lt;span style=\"margin-left: 0.3em\"&gt;Fran\u00e7ais&lt;/span&gt;\n      &lt;/a&gt;\n    &lt;/li&gt;\n    &lt;li class=\"lang-item lang-item-22 lang-item-de no-translation\"&gt;\n      &lt;a hreflang=\"de-DE\" href=\"https://example.org/de/\" lang=\"de-DE\"&gt;\n        &lt;img\n          src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAALCAIAAAD5gJpuAAABLElEQVR4AY2QgUZEQRSGz9ydmzbYkBWABBJYABHEFhJ6m0WP0DMEQNIr9AKrN8ne2Tt3Zs7MOdOZmRBEv+v34Tvub9R6fdNlAzU+snSME/wdjbjbbJ6EiEg6BA8102QbjKNpoMzw8v6qD/sOALbbT2MC1NgaAWOKOgxf5czY+4dbAX2G/THzcozLrvPV85IQyqVz0rvg2p9Pei4HjzSsiFbV4JgyhhxCjpGdZ0RhdikLB9/b8Qig7MkpSovR7Cp59q6CazaNFiTt4J82o6uvdMVwTsztKTXZod4jgOJJuqNAjFyGrBR8gM6XwKfIC4KanBSTZ0rClKh08D9DFh3egW7ebH7NcRDQWrz9rM2Ne+mDOXB2mZJ8agL19nwxR2iZXGm1gDbQKhDjd4yHb2oW/KR8xHicAAAAAElFTkSuQmCC\"\n          alt=\"Deutsch\"\n          style=\"width: 16px; height: 11px\"\n          width=\"16\"\n          height=\"11\"\n        /&gt;\n        &lt;span style=\"margin-left: 0.3em\"&gt;Deutsch&lt;/span&gt;\n      &lt;/a&gt;\n    &lt;/li&gt;\n    &lt;li class=\"lang-item lang-item-24 lang-item-es no-translation\"&gt;\n      &lt;a hreflang=\"es-ES\" href=\"https://example.org/es/\" lang=\"es-ES\"&gt;\n        &lt;img\n          src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAALCAMAAABBPP0LAAAAflBMVEX/AAD9AAD3AADxAADrAAD/eXn9bGz8YWH8WVn6UVH5SEj5Pz/3NDT0Kir9/QD+/nL+/lT18lDt4Uf6+j/39zD39yf19R3n5wDxflXsZ1Pt4Y3x8zr0wbLs1NXz8xPj4wD37t3jmkvsUU/Bz6nrykm3vJ72IiL0FBTyDAvhAABEt4UZAAAAX0lEQVR4AQXBQUrFQBBAwXqTDkYE94Jb73+qfwVRcYxVQRBRToiUfoaVpGTrtdS9SO0Z9FR9lVy/g5c99+dKl30N5uxPuviexXEc9/msC7TOkd4kHu/Dlh4itCJ8AP4B0w4Qwmm7CFQAAAAASUVORK5CYII=\"\n          alt=\"Espa\u00f1ol\"\n          style=\"width: 16px; height: 11px\"\n          width=\"16\"\n          height=\"11\"\n        /&gt;\n        &lt;span style=\"margin-left: 0.3em\"&gt;Espa\u00f1ol&lt;/span&gt;\n      &lt;/a&gt;\n    &lt;/li&gt;\n    &lt;li class=\"lang-item lang-item-26 lang-item-zh no-translation\"&gt;\n      &lt;a hreflang=\"zh-CN\" href=\"https://example.org/zh/\" lang=\"zh-CN\"&gt;\n        &lt;img\n          src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAALCAMAAABBPP0LAAAAXVBMVEXUAADlQgDLAADBAADtgXn63Xjypnf1wHHpcG/oZmbmXVzlU1PjS0q1AAD981775VvwnVD2zkvhPz/fNzfdMjHcKyvaJyfsi0baISHYGhqqAADWExPTDQ2jAACfAAApGpDBAAAAWklEQVR4ATXIhQHDQBTDUMll2n/RMiU5/vQsAE4EsPbaKVOU+pXNwc/WKQXeDZMKu+psCXw/Z7efarmENd6GIwGpXhUvM4spxoiEbouRNT7Fmtaq+RG4wAqZZvceD8DeIelqAAAAAElFTkSuQmCC\"\n          alt=\"\u4e2d\u6587 (\u4e2d\u56fd)\"\n          style=\"width: 16px; height: 11px\"\n          width=\"16\"\n          height=\"11\"\n        /&gt;\n        &lt;span style=\"margin-left: 0.3em\"&gt;\u4e2d\u6587 (\u4e2d\u56fd)&lt;/span&gt;\n      &lt;/a&gt;\n    &lt;/li&gt;\n    &lt;li class=\"lang-item lang-item-41 lang-item-ar no-translation\"&gt;\n      &lt;a hreflang=\"ar\" href=\"https://example.org/ar/\" lang=\"ar\"&gt;\n        &lt;img\n          src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAALCAMAAABBPP0LAAAANlBMVEUAYjMTYDs3R0AvV0NObzE3dSoTWzhAZjgyfEY0gl1EcDFqpIhKj28TVzaLs41ol1JSaF1JW1NzUHm9AAAAPUlEQVR4AY2MtQEAMAgE447tv2xKvuQqeEtRcikZ/9p6b9X/Mdfeaw4PnPvehQhNvpcnJYiInIqraqYpyAd1AAFxIEreLQAAAABJRU5ErkJggg==\"\n          alt=\"\u0627\u0644\u0639\u0631\u0628\u064a\u0629\"\n          style=\"width: 16px; height: 11px\"\n          width=\"16\"\n          height=\"11\"\n        /&gt;\n        &lt;span style=\"margin-left: 0.3em\"&gt;\u0627\u0644\u0639\u0631\u0628\u064a\u0629&lt;/span&gt;\n      &lt;/a&gt;\n    &lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/div&gt;\n</code></pre> </li> <li> <p>Adding to the navbar as a custom dropdown<sup>1</sup></p> Example <pre><code>&lt;div class=\"header-lang_switcher switcher-ltr\"&gt;\n  &lt;div class=\"current-lang-switcher\"&gt;\n    &lt;img src=\"https://example.org/flag_en.svg\" alt=\"flag-en\" /&gt;\n    &lt;span&gt;en&lt;/span&gt;\n  &lt;/div&gt;\n  &lt;ul&gt;\n    &lt;li class=\"lang-item lang-item-5 lang-item-fr lang-item-first\"&gt;\n      &lt;a\n        hreflang=\"fr-FR\"\n        href=\"https://example.org/fr/translation-page/\"\n        lang=\"fr-FR\"\n        &gt;Fran\u00e7ais&lt;/a\n      &gt;\n    &lt;/li&gt;\n    &lt;li class=\"lang-item lang-item-7 lang-item-de no-translation\"&gt;\n      &lt;a hreflang=\"de-DE\" href=\"https://example.org/de/\" lang=\"de-DE\"\n        &gt;Deutsch&lt;/a\n      &gt;\n    &lt;/li&gt;\n    &lt;li class=\"lang-item lang-item-9 lang-item-es no-translation\"&gt;\n      &lt;a hreflang=\"es-ES\" href=\"https://example.org/es/\" lang=\"es-ES\"\n        &gt;Espa\u00f1ol&lt;/a\n      &gt;\n    &lt;/li&gt;\n    &lt;li class=\"lang-item lang-item-11 lang-item-it no-translation\"&gt;\n      &lt;a hreflang=\"it-IT\" href=\"https://example.org/it/\" lang=\"it-IT\"\n        &gt;Italiano&lt;/a\n      &gt;\n    &lt;/li&gt;\n    &lt;li class=\"lang-item lang-item-13 lang-item-zh no-translation\"&gt;\n      &lt;a hreflang=\"zh-CN\" href=\"https://example.org/zh/\" lang=\"zh-CN\"\n        &gt;\u4e2d\u6587 (\u4e2d\u56fd)&lt;/a\n      &gt;\n    &lt;/li&gt;\n    &lt;li class=\"lang-item lang-item-15 lang-item-ar no-translation\"&gt;\n      &lt;a hreflang=\"ar\" href=\"https://example.org/ar/\" lang=\"ar\"\n        &gt;\u0627\u0644\u0639\u0631\u0628\u064a\u0629&lt;/a\n      &gt;\n    &lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/div&gt;\n</code></pre> </li> </ul> <p>Does not support:</p> <ul> <li>Methods which show the picker as a <code>&lt;select&gt;</code> element</li> </ul>"},{"location":"advanced/multilingual/#adding-support","title":"Adding Support","text":"<p>See also</p> <p>Using WPextract as a library for information on how to run wpextract as a library using additional pickers.</p> <p>Support can be added by creating a new picker definition inheriting from <code>LangPicker</code>.</p> <p>This parent class defines two abstract methods which must be implemented:</p> <ul> <li><code>LangPicker.get_root</code> - returns the root element of the picker</li> <li><code>LangPicker.extract</code> - find the languages, call <code>LangPicker.set_current_lang</code> and call <code>LangPicker.add_translation</code> for each</li> </ul> <p>More complicted pickers may need to override additional methods of the class, but should still ultimately populate the <code>LangPicker.translations</code> and <code>LangPicker.current_language</code> attributes as the parent class does.</p> <p>This section will show implementing a new picker with the following simplified markup:</p> <p><pre><code>&lt;ul class=\"translations\"&gt;\n  &lt;li&gt;&lt;a href=\"/page/\" class=\"lang current-lang\" lang=\"en\"&gt;English&lt;/a&gt;&lt;/li&gt;\n  &lt;li&gt;&lt;a href=\"/de/seite/\" class=\"lang\" lang=\"de\"&gt;Deutsch&lt;/a&gt;&lt;/li&gt;\n  &lt;li&gt;&lt;a href=\"/page/\" class=\"lang no-translation\" lang=\"fr\"&gt;Fran\u00e7ais&lt;/a&gt;&lt;/li&gt;\n&lt;/ul&gt;\n</code></pre> The correct parse of this picker should set the current language to English, add German as a translation, and ignore French.</p>"},{"location":"advanced/multilingual/#get_root","title":"<code>get_root()</code>","text":"<p>Using the <code>self.page_doc</code> attribute, a <code>BeautifulSoup</code> object representing the page, the root element of the picker should be found and returned.</p> <p>The <code>select_one</code> method is used to find the root element, and will return <code>None</code> if no element is found, which will be intepreted as the picker not being present on the page.</p> <p>If a value is returned, the <code>self.root_el</code> attribute will be populated with the result of this method.</p> Example <code>get_root</code> implementation <pre><code>class MyPicker(LangPicker):\n    ...\n    def get_root(self) -&gt; Tag:\n        return self.page_doc.select_one('ul', class_='translations')\n</code></pre>"},{"location":"advanced/multilingual/#extract","title":"<code>extract()</code>","text":"<p>Using the <code>self.root_el</code> attribute, the languages should be found and added to the dataset.</p> <p>Be careful to avoid: - Adding the current language - Adding languages which are listed but don't have translations</p> Example <code>extract</code> implementation <pre><code>class MyPicker(LangPicker):\n    ...\n    def extract(self):\n        for lang_el in self.root_el.select('li'):\n            lang_a = lang_el.select_one('a')\n            if 'current-lang' in lang_a.get('class'):\n                self.set_current_lang(lang)\n            elif 'no-translation' not in lang_a.get('class'):\n                self.add_translation(lang_a.get('href'), lang_a.get('lang'))\n</code></pre>"},{"location":"advanced/multilingual/#contributing-pickers","title":"Contributing Pickers","text":"<p>We welcome contributions via a GitHub PR so long as the picker is not overly specific to a single site. </p> <ol> <li> <p>This implementation may be overly customised to the site it was added to collect.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/downloader/","title":"Downloader API","text":""},{"location":"api/downloader/#downloading","title":"Downloading","text":""},{"location":"api/downloader/#extractor.WPDownloader","title":"extractor.WPDownloader","text":"<pre><code>WPDownloader(\n    target: str,\n    out_path: Path,\n    data_types: List[str],\n    session: Optional[RequestSession] = None,\n)\n</code></pre> <p>Manages the download of data from a WordPress site.</p> PARAMETER DESCRIPTION <code>target</code> <p>the target WordPress site URL</p> <p> TYPE: <code>str</code> </p> <code>out_path</code> <p>the output path for the downloaded data</p> <p> TYPE: <code>Path</code> </p> <code>data_types</code> <p>set of data types to download</p> <p> TYPE: <code>List[str]</code> </p> <code>session</code> <p>request session. Will be created from default constructor if not provided.</p> <p> DEFAULT: <code>None</code> </p>"},{"location":"api/downloader/#extractor.WPDownloader.download","title":"download","text":"<pre><code>download()\n</code></pre> <p>Download and export the requested data lists.</p>"},{"location":"api/downloader/#extractor.WPDownloader.download_media_files","title":"download_media_files","text":"<pre><code>download_media_files(session: RequestSession, dest: str)\n</code></pre> <p>Download site media files.</p> PARAMETER DESCRIPTION <code>session</code> <p>the request session to use</p> <p> TYPE: <code>RequestSession</code> </p> <code>dest</code> <p>destination directory for media</p> <p> TYPE: <code>str</code> </p>"},{"location":"api/downloader/#configuring-request-behaviour","title":"Configuring Request Behaviour","text":""},{"location":"api/downloader/#extractor.dl.RequestSession","title":"extractor.dl.RequestSession","text":"<pre><code>RequestSession(\n    proxy: str = None,\n    cookies: str = None,\n    authorization: AuthorizationType = None,\n    timeout: float = 30,\n    wait: float = None,\n    random_wait: bool = False,\n    max_retries: int = 10,\n    backoff_factor: float = 0.1,\n    max_redirects: int = 20,\n)\n</code></pre> <p>Wrapper to handle the requests library with session support</p> PARAMETER DESCRIPTION <code>proxy</code> <p>a dict containing a proxy server string for HTTP and/or HTTPS connection</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>cookies</code> <p>a string in the format of the Cookie header</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>authorization</code> <p>a tuple containing login and password or <code>requests.auth.HTTPBasicAuth</code> for basic authentication or <code>requests.auth.HTTPDigestAuth</code> for NTLM-like authentication</p> <p> TYPE: <code>AuthorizationType</code> DEFAULT: <code>None</code> </p> <code>timeout</code> <p>maximum time in seconds to wait for a response before giving up</p> <p> TYPE: <code>float</code> DEFAULT: <code>30</code> </p> <code>wait</code> <p>wait time in seconds between requests, None to not wait</p> <p> TYPE: <code>float</code> DEFAULT: <code>None</code> </p> <code>random_wait</code> <p>If true, the wait time between requests is multiplied by a random factor between 0.5 and 1.5</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>max_retries</code> <p>the maximum number of retries before failing</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>backoff_factor</code> <p>Factor to wait between successive retries</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>max_redirects</code> <p>maximum number of redirects to follow</p> <p> TYPE: <code>int</code> DEFAULT: <code>20</code> </p>"},{"location":"api/downloader/#extractor.dl.requestsession.AuthorizationType","title":"extractor.dl.requestsession.AuthorizationType  <code>module-attribute</code>","text":"<pre><code>AuthorizationType = Union[\n    Tuple[str, str], HTTPBasicAuth, HTTPDigestAuth\n]\n</code></pre>"},{"location":"api/extractor/","title":"Extractor API","text":""},{"location":"api/extractor/#extraction","title":"Extraction","text":""},{"location":"api/extractor/#extractor.WPExtractor","title":"extractor.WPExtractor","text":"<pre><code>WPExtractor(\n    json_root: Path,\n    scrape_root: Optional[Path] = None,\n    json_prefix: Optional[str] = None,\n    translation_pickers: Optional[PickerListType] = None,\n)\n</code></pre> <p>Manages the extraction of data from a WordPress site.</p> PARAMETER DESCRIPTION <code>json_root</code> <p>Path to directory of JSON files</p> <p> TYPE: <code>Path</code> </p> <code>scrape_root</code> <p>Path to scrape directory</p> <p> TYPE: <code>Optional[Path]</code> DEFAULT: <code>None</code> </p> <code>json_prefix</code> <p>Prefix of files in <code>json_root</code></p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>translation_pickers</code> <p>Supply a custom list of translation pickers</p> <p> TYPE: <code>Optional[PickerListType]</code> DEFAULT: <code>None</code> </p>"},{"location":"api/extractor/#extractor.WPExtractor.categories","title":"categories  <code>instance-attribute</code>","text":"<pre><code>categories: Optional[DataFrame]\n</code></pre> <p>DataFrame of extracted categories.</p>"},{"location":"api/extractor/#extractor.WPExtractor.link_registry","title":"link_registry  <code>instance-attribute</code>","text":"<pre><code>link_registry: LinkRegistry = LinkRegistry()\n</code></pre> <p>Registry of known URLs and their corresponding data items.</p>"},{"location":"api/extractor/#extractor.WPExtractor.media","title":"media  <code>instance-attribute</code>","text":"<pre><code>media: Optional[DataFrame]\n</code></pre> <p>DataFrame of extracted media.</p>"},{"location":"api/extractor/#extractor.WPExtractor.pages","title":"pages  <code>instance-attribute</code>","text":"<pre><code>pages: Optional[DataFrame]\n</code></pre> <p>DataFrame of extracted pages.</p>"},{"location":"api/extractor/#extractor.WPExtractor.posts","title":"posts  <code>instance-attribute</code>","text":"<pre><code>posts: Optional[DataFrame]\n</code></pre> <p>DataFrame of extracted posts.</p>"},{"location":"api/extractor/#extractor.WPExtractor.tags","title":"tags  <code>instance-attribute</code>","text":"<pre><code>tags: Optional[DataFrame]\n</code></pre> <p>DataFrame of extracted tags.</p>"},{"location":"api/extractor/#extractor.WPExtractor.users","title":"users  <code>instance-attribute</code>","text":"<pre><code>users: Optional[DataFrame]\n</code></pre> <p>DataFrame of extracted users.</p>"},{"location":"api/extractor/#extractor.WPExtractor.export","title":"export","text":"<pre><code>export(out_dir: Path) -&gt; None\n</code></pre> <p>Save scrape results!</p> PARAMETER DESCRIPTION <code>out_dir</code> <p>Path to output directory</p> <p> TYPE: <code>Path</code> </p>"},{"location":"api/extractor/#extractor.WPExtractor.extract","title":"extract","text":"<pre><code>extract() -&gt; None\n</code></pre> <p>Perform the extraction.</p>"},{"location":"api/extractor/#extraction-data","title":"Extraction Data","text":""},{"location":"api/extractor/#extractor.extractors.data.links.Link","title":"Link  <code>dataclass</code>","text":"<pre><code>Link(text: Optional[str], href: Optional[str])\n</code></pre> <p>A link to a URL.</p>"},{"location":"api/extractor/#extractor.extractors.data.links.LinkRegistry","title":"LinkRegistry","text":"<pre><code>LinkRegistry()\n</code></pre> <p>A collection of all known links on the site.</p>"},{"location":"api/extractor/#extractor.extractors.data.links.LinkRegistry.add_linkable","title":"add_linkable","text":"<pre><code>add_linkable(\n    url: str,\n    data_type: str,\n    idx: str,\n    _refresh_cache: bool = True,\n) -&gt; None\n</code></pre> <p>Add a single linkable item to the registry.</p> <p>The URL will be compared later against a list of links that need to be resolved and the data type and IDX will be returned.</p> <p>Data types should be unique. IDXes should be unique within one or more data types.</p> PARAMETER DESCRIPTION <code>url</code> <p>The URL of the destination</p> <p> TYPE: <code>str</code> </p> <code>data_type</code> <p>A unique identifier for this type of item.</p> <p> TYPE: <code>str</code> </p> <code>idx</code> <p>A unique identifier within the data type.</p> <p> TYPE: <code>str</code> </p> <code>_refresh_cache</code> <p>Whether the link cache should be updated. Should be left as True unless multiple links are being added together.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p>"},{"location":"api/extractor/#extractor.extractors.data.links.LinkRegistry.add_linkables","title":"add_linkables","text":"<pre><code>add_linkables(\n    data_type: str, links: List[str], idxes: List[str]\n) -&gt; None\n</code></pre> <p>Add multiple linkable items at once.</p> PARAMETER DESCRIPTION <code>data_type</code> <p>The data type for all items.</p> <p> TYPE: <code>str</code> </p> <code>links</code> <p>A list of links. Must be the same length as idxes.</p> <p> TYPE: <code>List[str]</code> </p> <code>idxes</code> <p>A list of IDs. Must be the same length as links.</p> <p> TYPE: <code>List[str]</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the links and idxes lists are not the same length.</p>"},{"location":"api/extractor/#extractor.extractors.data.links.LinkRegistry.query_link","title":"query_link","text":"<pre><code>query_link(href: str) -&gt; Optional[Linkable]\n</code></pre> <p>Find a linkable item by the URL in the registry.</p> <p>Returns None if no URL matches.</p> PARAMETER DESCRIPTION <code>href</code> <p>A URL to search</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[Linkable]</code> <p>A matching linkable</p>"},{"location":"api/extractor/#extractor.extractors.data.links.Linkable","title":"Linkable  <code>dataclass</code>","text":"<pre><code>Linkable(link: str, data_type: str, idx: str)\n</code></pre> <p>An item which can be linked to.</p>"},{"location":"api/extractor/#extractor.extractors.data.links.ResolvableLink","title":"ResolvableLink  <code>dataclass</code>","text":"<pre><code>ResolvableLink(\n    text: Optional[str],\n    href: Optional[str],\n    destination: Optional[Linkable],\n)\n</code></pre> <p>               Bases: <code>Link</code></p> <p>A link to a URL which can be looked up against known links.</p>"},{"location":"api/extractor/#multilingual-extraction","title":"Multilingual Extraction","text":""},{"location":"api/extractor/#extractor.parse.translations.LangPicker","title":"extractor.parse.translations.LangPicker","text":"<pre><code>LangPicker(page_doc: BeautifulSoup)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract class of a language picker style.</p> <p>Support for a new language picker can be added by creating a new class inheriting from this one.</p> PARAMETER DESCRIPTION <code>page_doc</code> <p>The document to extract a language picker from.</p> <p> TYPE: <code>BeautifulSoup</code> </p>"},{"location":"api/extractor/#extractor.parse.translations.LangPicker.current_language","title":"current_language  <code>instance-attribute</code>","text":"<pre><code>current_language: Language\n</code></pre> <p>The current language of the page, populated by calling <code>LangPicker.set_current_lang</code> within <code>LangPicker.extract</code>.</p>"},{"location":"api/extractor/#extractor.parse.translations.LangPicker.page_doc","title":"page_doc  <code>instance-attribute</code>","text":"<pre><code>page_doc: BeautifulSoup = page_doc\n</code></pre> <p>The document to extract the language picker from.</p>"},{"location":"api/extractor/#extractor.parse.translations.LangPicker.root_el","title":"root_el  <code>instance-attribute</code>","text":"<pre><code>root_el: Tag\n</code></pre> <p>The root element of the language picker, populated if <code>LangPicker.matches</code> is succesful.</p>"},{"location":"api/extractor/#extractor.parse.translations.LangPicker.translations","title":"translations  <code>instance-attribute</code>","text":"<pre><code>translations: List[TranslationLink] = []\n</code></pre> <p>A list of translation links, populated by calling <code>LangPicker.add_translation</code> within <code>LangPicker.extract</code>.</p>"},{"location":"api/extractor/#extractor.parse.translations.LangPicker.add_translation","title":"add_translation","text":"<pre><code>add_translation(href: str, lang: str) -&gt; None\n</code></pre> <p>Add a translation from the picker.</p> PARAMETER DESCRIPTION <code>href</code> <p>The link to the translated page.</p> <p> TYPE: <code>str</code> </p> <code>lang</code> <p>The provided language code.</p> <p> TYPE: <code>str</code> </p>"},{"location":"api/extractor/#extractor.parse.translations.LangPicker.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract() -&gt; None\n</code></pre> <p>Extract the current language and translations from the doc.</p>"},{"location":"api/extractor/#extractor.parse.translations.LangPicker.get_root","title":"get_root  <code>abstractmethod</code>","text":"<pre><code>get_root() -&gt; PageElement\n</code></pre> <p>Retrieve the root element of the translation picker.</p> <p>Using the <code>LangPicker.page_doc</code> attribute (a <code>bs4.BeautifulSoup</code> object representing the whole page), the root element of the picker shoudl be found and returned.</p> RETURNS DESCRIPTION <code>PageElement</code> <p>The root element, or None if this picker is not found on the page.</p>"},{"location":"api/extractor/#extractor.parse.translations.LangPicker.matches","title":"matches","text":"<pre><code>matches() -&gt; bool\n</code></pre> <p>Checks if this picker can extract from the document.</p> RETURNS DESCRIPTION <code>bool</code> <p>If the page uses this type of matcher.</p> RAISES DESCRIPTION <code>TypeError</code> <p>If the root element that has been retrieved is not a tag, or has 0 children. This may happen if it accidentally retrieves a text node.</p>"},{"location":"api/extractor/#extractor.parse.translations.LangPicker.set_current_lang","title":"set_current_lang","text":"<pre><code>set_current_lang(lang: str) -&gt; None\n</code></pre> <p>Set the language of this doc.</p> PARAMETER DESCRIPTION <code>lang</code> <p>The locale string</p> <p> TYPE: <code>str</code> </p>"},{"location":"api/extractor/#extractor.parse.translations.PickerListType","title":"extractor.parse.translations.PickerListType  <code>module-attribute</code>","text":"<pre><code>PickerListType = List[Type[LangPicker]]\n</code></pre>"},{"location":"api/extractor/#extractor.parse.translations.TranslationLink","title":"extractor.parse.translations.TranslationLink  <code>dataclass</code>","text":"<pre><code>TranslationLink(\n    text: Optional[str],\n    href: Optional[str],\n    destination: Optional[Linkable],\n    lang: str,\n)\n</code></pre> <p>               Bases: <code>ResolvableLink</code></p> <p>A link to an alternative version of this article in a different language.</p>"},{"location":"api/extractor/#extractor.parse.translations.TranslationLink.destination","title":"destination  <code>instance-attribute</code>","text":"<pre><code>destination: Optional[Linkable]\n</code></pre>"},{"location":"api/extractor/#extractor.parse.translations.TranslationLink.href","title":"href  <code>instance-attribute</code>","text":"<pre><code>href: Optional[str]\n</code></pre>"},{"location":"api/extractor/#extractor.parse.translations.TranslationLink.lang","title":"lang  <code>instance-attribute</code>","text":"<pre><code>lang: str\n</code></pre> <p>Raw language code.</p>"},{"location":"api/extractor/#extractor.parse.translations.TranslationLink.language","title":"language  <code>property</code>","text":"<pre><code>language: Language\n</code></pre> <p>Parsed and normalized language. Populated automatically post-init.</p>"},{"location":"intro/install/","title":"Installing","text":"<p>WPextract is available on the PyPI and Anaconda package indexes.</p> <p>The minimum supported Python version is 3.9. WPextract should work on any Python-supported operating system.</p>"},{"location":"intro/install/#with-pipx-recommended-for-end-users","title":"With <code>pipx</code> (recommended for end-users)","text":"<p>pipx will automatically install WPextract and its dependencies in an isolated yet globally-accessible environment.</p> <p>We recommend this method for those wishing to use WPextract through its command line interface.</p> <ol> <li>If pipx is not already installed, follow the steps in the pipx installation instructions</li> <li>Install WPextract:     <pre><code>$ pipx install wpextract\n</code></pre></li> <li>Test installation:     <pre><code>$ wpextract --help\n</code></pre></li> </ol> Upgrading <p>To upgrade versions use: <pre><code>$ pipx upgrade wpextract\n</code></pre></p> Multiple/Specific Versions <p>To install a specific version, use: <pre><code>$ pipx install wpextract==1.0.0\n</code></pre> Multiple versions can be installed in parallel using suffixes: <pre><code>$ pipx install --suffix=@1.0.0 wpextract==1.0.0\n$ pipx install --suffix=@1.1.0 wpextract==1.1.0\n$ wpextract@1.0.0 --version\n$ wpextract@1.1.0 --version\n</code></pre> The suffix is user-specified and does not necessarily have to match the version number.</p> Uninstalling <p>To uninstall, use: <pre><code>$ pipx uninstall wpextract\n</code></pre> This will cleanly uninstall the package and delete its environment</p>"},{"location":"intro/install/#with-pip-or-conda","title":"With <code>pip</code> or <code>conda</code>","text":"<p>To install within an existing environment, which is necessary to use WPextract as a library.</p> <p>WPextract can be manually installed with:</p> PipConda <pre><code>$ pip install wpextract\n</code></pre> <pre><code>$ conda install wpextract\n</code></pre> <p>The installation can be tested through the command-line interface:</p> <pre><code>$ wpextract --help\n</code></pre> <p>or through importing as a library:</p> <pre><code>&gt;&gt;&gt; import wpextract\n&gt;&gt;&gt; wpextract.__version__\n1.0.0\n</code></pre>"},{"location":"intro/install/#for-development","title":"For Development","text":"<p>Dependencies for development and packaging are managed through Poetry.</p> <p>Poetry will automatically install dependencies and the package itself in editable mode:</p> <pre><code>$ poetry install\n$ poetry install --with docs # to build docs locally\n$ wpextract --help\n</code></pre>"},{"location":"intro/start/","title":"Getting Started","text":"<p>In this guide we will use the command-line interface (CLI) of WPextract to download a copy of a WordPress site and produce a usable cross-referenced dataset. Afterwards, you should be ready to use WPextract for your own research or archiving.</p> <p>Be considerate when using this tool.</p> <p>It is your responsibility to ensure any usage of this tool is ethical and compliant with the law. In general, you should only scrape sites with permission of the owner, or for appropriate research following your institution's ethical guidelines.</p> <p>You should use the options included in the <code>dl</code> command to minimise the impact of this tool on servers.</p>"},{"location":"intro/start/#installation","title":"Installation","text":"<p>See the installation instructions for requirements and alternative methods.</p> <p>WPExtract can be quickly installed in a sandboxed environment with pipx:</p> <pre><code>$ pipx install wpextract\n</code></pre>"},{"location":"intro/start/#wpextract-stages","title":"WPextract Stages","text":"<p>WPextract works in two steps:</p> <ol> <li>The downloader uses the WordPress REST API to obtain all content on the site, which is stored as a single, long file</li> <li>The extractor converts this into a usable dataset by enriching the downloaded content. This includes extracting text, images, resolving links to posts/pages, and finding translated versions<sup>1</sup></li> </ol> <p>We call these two stages using two CLI commands (<code>wpextract dl</code> and <code>wpextract extract</code>). Alternatively, WPExtract can be integrated into a project by using it as a library.</p>"},{"location":"intro/start/#1-downloading","title":"1. Downloading","text":"<p>For the purposes of this guide, we'll use the URL <code>https://example.org</code> - in reality this isn't a WordPress site, so you should replace it with the URL of the site you're interested in.</p> <p>To download the contents of <code>https://example.org</code> to the directory <code>./example.org</code>, run:</p> <pre><code>$ wpextract dl \"https://example.org\" ./example.org\n</code></pre> <p>Progress bars will be displayed as it iterates through the pages of each content type until completed.</p> <p>The <code>./example.org</code> directory will now contain 6 JSON files containing the categories, media, pages, posts, tags and users data. These are simply all the pages retrieved from the API, concatenated together.</p> <p>Using this data directly has several issues:</p> <ul> <li>Post and page content is provided as rendered HTML - although this is cleaner than scraped HTML, it may still contain intermingled content like images.</li> <li>No information on links or media - links between posts and pages and media used are only available as <code>&lt;a&gt;</code>/<code>&lt;img&gt;</code> tags within the source, so can't be directly used for analysis</li> <li>Data may be missing - Data from some plugins may not be present in the API response, so an HTML scrape needs to be performed and somehow connected to the API response.</li> </ul> <p>To rectify this, we use the extract command to build a usable dataset.</p>"},{"location":"intro/start/#2-extraction","title":"2. Extraction","text":"<p>Now, we call the second command passing in the output directory from the first (<code>./example.org</code>) and an output directory for our data (<code>./example.org-data</code>)</p> <pre><code>$ wpextract extract ./example.org ./example.org-data\n</code></pre> <p>This will again output 6 JSON files containing a modified version of the WordPress API schema. This includes:</p> <ul> <li>plain text content for posts and pages with normalised paragraph breaks and no stray inline content</li> <li>internal links are resolved to the type of data (post, page, media) and its ID</li> <li>media is resolved to its associated media ID</li> <li>embeds (<code>&lt;iframe&gt;</code> tags, e.g. YouTube videos) are listed</li> <li>many unnecessary fields are removed</li> <li>some fields provided by plugins are kept and used if available</li> </ul>"},{"location":"intro/start/#further-information","title":"Further Information","text":"<p>This guide only covered the basic use-case of WPextract, for further information see:</p> <ul> <li>More comprehensive documentation of the download and extract commands</li> <li>How to extract multilingual data</li> </ul> <ol> <li> <p>{-} See the specific guide for more on multilingual extraction.\u00a0\u21a9</p> </li> </ol>"},{"location":"intro/why/","title":"Why WPextract?","text":"<p>Scraping websites for research is a common practice in the Natural Language Processing, Computational Social Science, and Digital Humanities fields.</p> <p>A typical scraping operation might be performed recursively using a tool such as <code>wget</code> or HTTrack, where the homepage of the site is visited, then all pages linked on the homepage, then all pages linked on those pages, and so on to a maximum depth. Each page's HTML is downloaded and stored in a structure resembling its URL.</p> <p>This approach presents several issues:</p> <ul> <li>Only pages reachable within a certain depth will be saved - this can be particularly problematic for blogs where older articles require navigation through several pages of archives to access.</li> <li>The downloaded HTML must then be parsed to extract the desired content - this can be complex and tiresome, and may require different implementations for different websites</li> <li>Only metadata exposed on the page is collected - or if the site is operated by a bad actor (such as in disinformation study), the metadata shown on the page may be false</li> </ul> <p>Some other approaches partially resolve these issues, like using a sitemap to obtain a list of all webpages, but this is still imperfect.</p>"},{"location":"intro/why/#wordpress","title":"WordPress","text":"<p>WordPress is a popular open-source content management system (CMS). It is widely popular due to the availability of cheap hosting, expertise, and a wide range of themes and plugins. Therefore, inevitably, it is a popular tool for bad actors.</p> <p>WordPress exposes a REST API, which can be used to list the posts, pages, and other content types of the site. Using this API instead of an HTML scrape:</p> <ul> <li>All content can be obtained, even content which isn't linked to anywhere</li> <li>Clean content is provided exactly as was entered into the editor box</li> <li>Additional metadata is available, including precise timestamps, incremental IDs, exact authorship data, and more.</li> </ul>"},{"location":"intro/why/#limitations","title":"Limitations","text":"<ul> <li>Only WordPress sites are supported - this is an inherent limitation of using the WordPress API</li> <li>Some security/performance plugins may block the API</li> <li>Certain types of data are not present in the API - e.g. multilingual data, some comment plugins.</li> <li>Custom data types will not be extracted - the tool specifically works on the standard WordPress data types. Even if a plugin exposes a new data type in the same way, it will not be extracted.</li> </ul>"},{"location":"intro/why/#use-in-research","title":"Use in Research","text":"<p>WPextract was built initially to study two state-backed propaganda sites, subsequently published in the paper Analysing State-Backed Propaganda Websites: a New Dataset and Linguistic Study at EMNLP 2023.</p> <p>In this example, a disinformation article is shown. On the left, various potential parts of interest are highlighted, which would require relatively complex parsing written specifically for this site. On the right, the data extracted by WPextract via the API is shown. This includes data not present on the page (modification time) or obscured by the site owners (author).</p> <p></p>"},{"location":"usage/download/","title":"Download Command","text":"<p>The <code>wpextract dl</code> command downloads the content of a site using the REST API.</p>"},{"location":"usage/download/#command-usage","title":"Command Usage","text":"<pre><code>$ wpextract dl target out_json\n</code></pre> <code>target</code> The HTTP(S) URL of the WordPress site. <code>out_json</code> Directory to output to <p>skip data</p> <code>--no-categories</code> <code>--no-media</code> <code>--no-pages</code> <code>--no-posts</code> <code>--no-tags</code> <code>--no-users</code> Skip downloading the given data type <p>authentication</p> <code>--auth AUTH</code> Define HTTP Basic credentials in format username:password <code>--cookies COOKIES</code> Define cookies to send with request in the format \"cookie1=foo; cookie2=bar\" <code>--proxy PROXY</code> Define a proxy server to use <p>request behaviour</p> <code>--timeout TIMEOUT</code> Stop waiting for a response after a given number of seconds (default: 30) <code>--wait WAIT</code> Wait the specified number of seconds between retrievals (default: None) <code>--random-wait</code> Randomly varies the time between requests to between 0.5 and 1.5 times the number of seconds set by \u2013wait <code>--max-retries MAX_RETRIES</code> Maximum number of retries before giving up (default: 10) <code>--backoff-factor BACKOFF_FACTOR</code> Factor to apply delaying retries. Default will sleep for 0.0, 0.2, 0.4, 0.8,\u2026 (default: 0.1) <code>--max-redirects MAX_REDIRECTS</code> Maximum number of redirects before giving up (default: 20) <p>logging</p> <code>--log LOG</code>, <code>-l LOG</code> Log outputs to this file instead of stdout. <code>--verbose</code>, <code>-v</code> Show additional debug logs"},{"location":"usage/download/#download-process","title":"Download Process","text":"<p>For each enabled data type (categories, media, pages, posts, tags, users; all by default), the command will use the REST API to download the data. The API is paginated and the command will show a progress bar for each page of data.</p>"},{"location":"usage/download/#endpoints","title":"Endpoints","text":"<p>To produce each file, the following list endpoints are used:</p> File Name Endpoint <code>categories.json</code> <code>/wp/v2/categories</code> <code>comments.json</code> <code>/wp/v2/comments</code> <code>media.json</code> <code>/wp/v2/media</code> <code>pages.json</code> <code>/wp/v2/pages</code> <code>posts.json</code> <code>/wp/v2/posts</code> <code>tags.json</code> <code>/wp/v2/tags</code> <code>users.json</code> <code>/wp/v2/users</code>"},{"location":"usage/download/#bot-protection-and-considerate-scraping","title":"Bot Protection and Considerate Scraping","text":"<p>It's unlikely this will trigger bot protection mechanisms for the following reasons:</p> <ul> <li>it is accessing intended API endpoints, which are likely to have lower levels of bot protection</li> <li>it has been configured to use a browser user agent</li> </ul> <p>The following measures are taken to be considerate to the server:</p> <ul> <li>a backoff factor is applied to retries</li> </ul> <p>We would also suggest enabling the following options, with consideration for how they will affect the download speed:</p> <ul> <li><code>--wait</code> to space out requests</li> <li><code>--random-wait</code> to vary the time between requests to avoid patterns</li> </ul>"},{"location":"usage/download/#error-handling","title":"Error Handling","text":"<p>If an HTTP error occurs, the command will retry the request up to <code>--max-retries</code> times, with the backoff set by <code>--backoff-factor</code>. If the maximum number of retries is reached, the command will output the error, stop collecting the given data type, and start collecting the following data type. This is because it's presumed that if a given page is non-functional, the following one will be too.</p> <p>To ensure the integrity of the scrape, it is suggested to check the logs for errors afterwards.</p> <p>There is currently no mechanism to resume interrupted downloads.</p>"},{"location":"usage/extract/","title":"Extract Command","text":"<p>The <code>wpextract extract</code> command converts the WordPress API response format into a usable dataset for downstream tasks.</p>"},{"location":"usage/extract/#command-usage","title":"Command Usage","text":"<pre><code>$ wpextract extract json_root out_dir\n</code></pre> <code>json_root</code> Path to files generated by the <code>wpextract dl</code> command <code>out_dir</code> Output directory for generated dataset. This should be different to <code>json_root</code> as it will create files with the same name. <p>optional arguments</p> <code>--scrape-root SCRAPE_ROOT</code> Root directory of an HTML scrape, see scrape crawling. <code>--json-prefix JSON_PREFIX</code> Load and output files with a prefix, e.g. supplying 20240101-example will output posts to <code>out_dir/20240101-example-posts.json</code> <p>logging</p> <code>--log LOG</code>, <code>-l LOG</code> Log outputs to this file instead of stdout. <code>--verbose</code>, <code>-v</code> Show additional debug logs"},{"location":"usage/extract/#extraction-process","title":"Extraction Process","text":""},{"location":"usage/extract/#1-scrape-crawling-optional","title":"1. Scrape Crawling (optional)","text":"<p>If a scrape is provided with the <code>--scrape-root</code> argument, it is first crawled to map the correspondance between the HTML files on disk and the post URLs.</p> <p>Website scraping tools may store a webpage at a path that is not easy to derive from the URL (e.g. because of path length limits). For this reason, we crawl the scrape directory and build a mapping of URL to path.</p> <p>For every html file at any depth in the scrape directory, we:</p> <ol> <li>Perform a limited parse of only the link and meta tags in the file's head.</li> <li>Attempt to extract a valid URL from a <code>link</code> tag with <code>rel=\"alternate\"</code> or <code>canonical</code> meta tag</li> <li>Check the URL has not previously been seen, warn and skip if it has</li> <li>Add the URL to the map with the absolute path of the file</li> </ol> <p>This map is then saved as <code>url_cache.json</code> in the scrape directory. If an existing cache file is detected, it will be used instead of scraping again, unless a breaking change has been made to the file schema.</p>"},{"location":"usage/extract/#2-content-extraction","title":"2. Content Extraction","text":"<p>Each type of content (posts, pages, media etc) is now extracted in turn.</p> <p>The extraction process is applied to all posts simultaneously in the following order:</p> <ol> <li>Extract raw text from the HTML-formatted title and excerpt.</li> <li>Parse the HTML content from the API response input.</li> <li>Parse the HTML content from the scrape file, if it was found for the link during the crawl</li> <li>Extract the post's language and translations from the scrape file</li> <li>Translations are detected using the translation pickers (implementing <code>LangPicker</code>)</li> <li>Custom pickers can be added if using this tool as a library</li> <li>Any extracted translations are stored as unresolved links</li> <li>Add the post's link to the link registry</li> <li>Using the parsed API content response, extract:</li> <li>Internal links (stored as unresolved links)</li> <li>External links (stored as resolved links)</li> <li>Embeds (<code>iframe</code> tags)</li> <li>Images (stored as unresolved media if internal, resolved media if external), including their source URL, alt text and caption (if they are in a <code>figure</code>)</li> <li>Raw text content, via the following process:<ol> <li>Remove tags which contain unwanted text (e.g. <code>figcaptions</code>)</li> <li>Replace <code>&lt;br&gt;</code> tags and <code>&lt;p&gt;</code> tags with newline characters</li> <li>Combine all page text</li> </ol> </li> </ol> <p>Other types are extracted in similar ways. Any additional user-supplied fields with HTML formatting (such as media captions) are also extracted as plain text.</p>"},{"location":"usage/extract/#4-translation-normalisation-and-link-resolution","title":"4. Translation Normalisation and Link Resolution","text":"<p>Translations are normalised by checking that for every translation relation (e.g. <code>en</code> -&gt; <code>fr</code>), the reverse exists. If not, it will be added.</p> <p>After all types have been processed, the link registry is used to process the unresolved links, translations and media.</p> <p>For every resolution, the following steps are performed:</p> <ol> <li>Remove the <code>preview_id</code> query parameter from the URL if present</li> <li>Attempt to look up the URL in the link registry</li> <li>If unsuccessful, use a heuristic to detect category slugs in the URL and try without them</li> <li>We do this in case sites have removed category slugs from the permalink at some point.</li> <li>If unsuccessful, warn that the URL is unresolvable</li> </ol> <p>For each resolved link, translation, or media, a destination is set containing its normalised URL, data type, and ID.</p>"},{"location":"usage/extract/#5-export","title":"5. Export","text":"<p>The columns of each type are subset and exported as a JSON file each to the specified output path, using a prefix if supplied.</p>"}]}